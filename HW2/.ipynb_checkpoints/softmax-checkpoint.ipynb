{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the softmax workbook for ECE 239AS Assignment #2\n",
    "\n",
    "Please follow the notebook linearly to implement a softmax classifier.\n",
    "\n",
    "Please print out the workbook entirely when completed.\n",
    "\n",
    "We thank Serena Yeung & Justin Johnson for permission to use code written for the CS 231n class (cs231n.stanford.edu).  These are the functions in the cs231n folders and code in the jupyer notebook to preprocess and show the images.  The classifiers used are based off of code prepared for CS 231n as well.\n",
    "\n",
    "The goal of this workbook is to give you experience with training a softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a softmax classifier.\n",
    "\n",
    "The following cells will take you through building a softmax classifier.  You will implement its loss function, then subsequently train it with gradient descent.  Finally, you will choose the learning rate of gradient descent to optimize its classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nndl import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare an instance of the Softmax class.  \n",
    "# Weights are initialized to a random value.\n",
    "# Note, to keep people's first solutions consistent, we are going to use a random seed.\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "softmax = Softmax(dims=[num_classes, num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implement the loss function of the softmax using a for loop over\n",
    "#  the number of examples\n",
    "\n",
    "loss = softmax.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3277607028\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "You'll notice the loss returned by the softmax is about 2.3 (if implemented correctly).  Why does this value make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Because we have 10 classes in this case. Before training this model, the probability of every class to be choosen is the same. That is 1/10. Hence by using loss function, the loss will be -log(1/10) which is 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.323420 analytic: 0.323420, relative error: 4.340850e-08\n",
      "numerical: 2.109268 analytic: 2.109267, relative error: 1.393414e-08\n",
      "numerical: -0.765416 analytic: -0.765417, relative error: 2.377212e-08\n",
      "numerical: 2.323441 analytic: 2.323441, relative error: 1.049916e-08\n",
      "numerical: 1.318364 analytic: 1.318364, relative error: 4.979324e-08\n",
      "numerical: 0.738926 analytic: 0.738925, relative error: 5.682309e-08\n",
      "numerical: -0.739583 analytic: -0.739583, relative error: 5.397605e-08\n",
      "numerical: 0.292931 analytic: 0.292931, relative error: 7.089985e-08\n",
      "numerical: 0.903963 analytic: 0.903963, relative error: 3.482681e-10\n",
      "numerical: -2.642383 analytic: -2.642383, relative error: 1.297226e-08\n"
     ]
    }
   ],
   "source": [
    "## Calculate the gradient of the softmax loss in the Softmax class.\n",
    "# For convenience, we'll write one function that computes the loss\n",
    "#   and gradient together, softmax.loss_and_grad(X, y)\n",
    "# You may copy and paste your loss code from softmax.loss() here, and then\n",
    "#   use the appropriate intermediate values to calculate the gradient.\n",
    "\n",
    "loss, grad = softmax.loss_and_grad(X_dev,y_dev)\n",
    "\n",
    "# Compare your gradient to a gradient check we wrote. \n",
    "# You should see relative gradient errors on the order of 1e-07 or less if you implemented the gradient correctly.\n",
    "softmax.grad_check_sparse(X_dev, y_dev, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A vectorized version of Softmax\n",
    "\n",
    "To speed things up, we will vectorize the loss and gradient calculations.  This will be helpful for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal loss / grad_norm: 2.2611884461603817 / 256.55325524779994 computed in 0.04638171195983887s\n",
      "Vectorized loss / grad: 2.261188446160382 / 256.55325524779994 computed in 0.004354953765869141s\n",
      "difference in loss / grad: -4.440892098500626e-16 /2.3184902617331804e-13 \n"
     ]
    }
   ],
   "source": [
    "## Implement softmax.fast_loss_and_grad which calculates the loss and gradient\n",
    "#    WITHOUT using any for loops.  \n",
    "\n",
    "# Standard loss and gradient\n",
    "tic = time.time()\n",
    "loss, grad = softmax.loss_and_grad(X_dev, y_dev)\n",
    "toc = time.time()\n",
    "print('Normal loss / grad_norm: {} / {} computed in {}s'.format(loss, np.linalg.norm(grad, 'fro'), toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax.fast_loss_and_grad(X_dev, y_dev)\n",
    "toc = time.time()\n",
    "print('Vectorized loss / grad: {} / {} computed in {}s'.format(loss_vectorized, np.linalg.norm(grad_vectorized, 'fro'), toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('difference in loss / grad: {} /{} '.format(loss - loss_vectorized, np.linalg.norm(grad - grad_vectorized)))\n",
    "\n",
    "# You should notice a speedup with the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "We now implement stochastic gradient descent.  This uses the same principles of gradient descent we discussed in class, however, it calculates the gradient by only using examples from a subset of the training set (so each gradient calculation is faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "How should the softmax gradient descent training step differ from the svm training step, if at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "The way the softmax gradient training step and that of svm is the same. Both use the idea of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 2.336592660663754\n",
      "iteration 100 / 1500: loss 2.055722261347679\n",
      "iteration 200 / 1500: loss 2.0357745120763413\n",
      "iteration 300 / 1500: loss 1.9813348165594475\n",
      "iteration 400 / 1500: loss 1.9583142444066215\n",
      "iteration 500 / 1500: loss 1.8622653073105446\n",
      "iteration 600 / 1500: loss 1.8532611454033445\n",
      "iteration 700 / 1500: loss 1.835306222323703\n",
      "iteration 800 / 1500: loss 1.829389246837283\n",
      "iteration 900 / 1500: loss 1.8992158530300574\n",
      "iteration 1000 / 1500: loss 1.9783503540803116\n",
      "iteration 1100 / 1500: loss 1.8470797913059949\n",
      "iteration 1200 / 1500: loss 1.8411450268693184\n",
      "iteration 1300 / 1500: loss 1.7910402495385682\n",
      "iteration 1400 / 1500: loss 1.8705803029813195\n",
      "That took 3.0658533573150635s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQdARpSAIgiDo/D8I5wjH8UGNkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1111971d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement softmax.train() by filling in the code to extract a batch of data\n",
    "# and perform the gradient step.\n",
    "import time\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took {}s'.format(toc - tic))\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the trained softmax classifier on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.3811428571428571\n",
      "validation accuracy: 0.398\n"
     ]
    }
   ],
   "source": [
    "## Implement softmax.predict() and use it to compute the training and testing error.\n",
    "\n",
    "y_train_pred = softmax.predict(X_train)\n",
    "print('training accuracy: {}'.format(np.mean(np.equal(y_train,y_train_pred), )))\n",
    "y_val_pred = softmax.predict(X_val)\n",
    "print('validation accuracy: {}'.format(np.mean(np.equal(y_val, y_val_pred)), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the softmax classifier\n",
    "\n",
    "You may copy and paste your optimization code from the SVM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2204460492503131e-16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 2.2888564871800297\n",
      "iteration 100 / 1500: loss 33.048102666745365\n",
      "iteration 200 / 1500: loss 16.46821997275276\n",
      "iteration 300 / 1500: loss 25.100869938358905\n",
      "iteration 400 / 1500: loss 38.079963013889056\n",
      "iteration 500 / 1500: loss 17.3701366375199\n",
      "iteration 600 / 1500: loss 27.732217308889975\n",
      "iteration 700 / 1500: loss 39.0332938179829\n",
      "iteration 800 / 1500: loss 25.071663716943117\n",
      "iteration 900 / 1500: loss 28.27916388409148\n",
      "iteration 1000 / 1500: loss 26.462798316540795\n",
      "iteration 1100 / 1500: loss 29.336807248922064\n",
      "iteration 1200 / 1500: loss 28.50389380725291\n",
      "iteration 1300 / 1500: loss 34.867229682167284\n",
      "iteration 1400 / 1500: loss 38.90287158642168\n",
      "learning_rate: 0.0001 training accuracy: 0.3006938775510204\n",
      "learning_rate: 0.0001 validation accuracy: 0.291\n",
      "iteration 0 / 1500: loss 2.3193958424912204\n",
      "iteration 100 / 1500: loss 2.7958669979809248\n",
      "iteration 200 / 1500: loss 3.3521150446484773\n",
      "iteration 300 / 1500: loss 2.4001322421375217\n",
      "iteration 400 / 1500: loss 3.1167136752231897\n",
      "iteration 500 / 1500: loss 2.5012723865000246\n",
      "iteration 600 / 1500: loss 4.338245057483891\n",
      "iteration 700 / 1500: loss 2.443815830266712\n",
      "iteration 800 / 1500: loss 2.003625646113254\n",
      "iteration 900 / 1500: loss 3.1573707708086274\n",
      "iteration 1000 / 1500: loss 2.3278322274227974\n",
      "iteration 1100 / 1500: loss 2.3351037967854893\n",
      "iteration 1200 / 1500: loss 2.278294438880007\n",
      "iteration 1300 / 1500: loss 2.838200876138562\n",
      "iteration 1400 / 1500: loss 3.160178911078196\n",
      "learning_rate: 1e-05 training accuracy: 0.31873469387755105\n",
      "learning_rate: 1e-05 validation accuracy: 0.288\n",
      "iteration 0 / 1500: loss 2.340555837844859\n",
      "iteration 100 / 1500: loss 1.8273894854345532\n",
      "iteration 200 / 1500: loss 1.7075885024618633\n",
      "iteration 300 / 1500: loss 1.7394053260499778\n",
      "iteration 400 / 1500: loss 1.847613803870957\n",
      "iteration 500 / 1500: loss 1.785075236919551\n",
      "iteration 600 / 1500: loss 1.6944376471546345\n",
      "iteration 700 / 1500: loss 1.7468805790394668\n",
      "iteration 800 / 1500: loss 1.6786762193599531\n",
      "iteration 900 / 1500: loss 1.7738606000131554\n",
      "iteration 1000 / 1500: loss 1.7175673062127534\n",
      "iteration 1100 / 1500: loss 1.6333217509569693\n",
      "iteration 1200 / 1500: loss 1.714796601474586\n",
      "iteration 1300 / 1500: loss 1.5966571409812622\n",
      "iteration 1400 / 1500: loss 1.796877729372034\n",
      "learning_rate: 1e-06 training accuracy: 0.4215510204081633\n",
      "learning_rate: 1e-06 validation accuracy: 0.403\n",
      "iteration 0 / 1500: loss 2.373376401593462\n",
      "iteration 100 / 1500: loss 2.0440373663433116\n",
      "iteration 200 / 1500: loss 1.9896305211253664\n",
      "iteration 300 / 1500: loss 1.9760502823881314\n",
      "iteration 400 / 1500: loss 1.890116235211377\n",
      "iteration 500 / 1500: loss 1.9562073168577803\n",
      "iteration 600 / 1500: loss 1.8845209778292746\n",
      "iteration 700 / 1500: loss 1.8331865726610732\n",
      "iteration 800 / 1500: loss 1.8228906796247122\n",
      "iteration 900 / 1500: loss 1.8268067660228664\n",
      "iteration 1000 / 1500: loss 1.806922628396179\n",
      "iteration 1100 / 1500: loss 1.7622499432592418\n",
      "iteration 1200 / 1500: loss 1.7925869557087752\n",
      "iteration 1300 / 1500: loss 1.8258958712618079\n",
      "iteration 1400 / 1500: loss 1.7595333056912412\n",
      "learning_rate: 1e-07 training accuracy: 0.382530612244898\n",
      "learning_rate: 1e-07 validation accuracy: 0.394\n",
      "iteration 0 / 1500: loss 2.3746794685487482\n",
      "iteration 100 / 1500: loss 2.24741547190601\n",
      "iteration 200 / 1500: loss 2.2345753278649805\n",
      "iteration 300 / 1500: loss 2.1501536411883335\n",
      "iteration 400 / 1500: loss 2.13948351972185\n",
      "iteration 500 / 1500: loss 2.110029318287606\n",
      "iteration 600 / 1500: loss 2.140012608412966\n",
      "iteration 700 / 1500: loss 2.134568030649806\n",
      "iteration 800 / 1500: loss 2.0239545288085377\n",
      "iteration 900 / 1500: loss 2.0577949309379266\n",
      "iteration 1000 / 1500: loss 1.9581015071649717\n",
      "iteration 1100 / 1500: loss 2.079269156379364\n",
      "iteration 1200 / 1500: loss 2.09537294634235\n",
      "iteration 1300 / 1500: loss 2.1184080715120426\n",
      "iteration 1400 / 1500: loss 1.9932848573922808\n",
      "learning_rate: 1e-08 training accuracy: 0.2939591836734694\n",
      "learning_rate: 1e-08 validation accuracy: 0.319\n",
      "iteration 0 / 1500: loss 2.357655380627855\n",
      "iteration 100 / 1500: loss 2.3330505218748336\n",
      "iteration 200 / 1500: loss 2.2705398503629244\n",
      "iteration 300 / 1500: loss 2.3007071173430655\n",
      "iteration 400 / 1500: loss 2.2911486443847426\n",
      "iteration 500 / 1500: loss 2.288508984543592\n",
      "iteration 600 / 1500: loss 2.2833701263341184\n",
      "iteration 700 / 1500: loss 2.2634148269095142\n",
      "iteration 800 / 1500: loss 2.268346801457455\n",
      "iteration 900 / 1500: loss 2.257932142290223\n",
      "iteration 1000 / 1500: loss 2.2789923328353496\n",
      "iteration 1100 / 1500: loss 2.2486640139637317\n",
      "iteration 1200 / 1500: loss 2.266767770250993\n",
      "iteration 1300 / 1500: loss 2.246205363895815\n",
      "iteration 1400 / 1500: loss 2.2507304888910835\n",
      "learning_rate: 1e-09 training accuracy: 0.16753061224489796\n",
      "learning_rate: 1e-09 validation accuracy: 0.176\n"
     ]
    }
   ],
   "source": [
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#   Train the Softmax classifier with different learning rates and \n",
    "#     evaluate on the validation data.\n",
    "#   Report:\n",
    "#     - The best learning rate of the ones you tested.  \n",
    "#     - The best validation accuracy corresponding to the best validation error.\n",
    "#\n",
    "#   Select the SVM that achieved the best validation error and report\n",
    "#     its error rate on the test set.\n",
    "# ================================================================ #\n",
    "learning_rates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "for learning in learning_rates:\n",
    "    softmax.train(X_train, y_train, learning_rate=learning, num_iters=1500, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    print('learning_rate: {} training accuracy: {}'.format(learning, np.mean(np.equal(y_train,y_train_pred), )))\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    print('learning_rate: {} validation accuracy: {}'.format(learning, np.mean(np.equal(y_val, y_val_pred)), ))\n",
    "\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The softmax.py is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Softmax(object):\n",
    "\n",
    "  def __init__(self, dims=[10, 3073]):\n",
    "    self.init_weights(dims=dims)\n",
    "\n",
    "  def init_weights(self, dims):\n",
    "    \"\"\"\n",
    "\tInitializes the weight matrix of the Softmax classifier.  \n",
    "\tNote that it has shape (C, D) where C is the number of \n",
    "\tclasses and D is the feature size.\n",
    "\t\"\"\"\n",
    "    self.W = np.random.normal(size=dims) * 0.0001\n",
    "\n",
    "  def loss(self, X, y):\n",
    "    \"\"\"\n",
    "    Calculates the softmax loss.\n",
    "  \n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "  \n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "  \n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the loss to zero.\n",
    "    loss = 0.0\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the normalized softmax loss.  Store it as the variable loss.\n",
    "    #   (That is, calculate the sum of the losses of all the training \n",
    "    #   set margins, and then normalize the loss by the number of \n",
    "    #   training examples.)\n",
    "    # ================================================================ #\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        ax_i = self.W.dot(X[i])\n",
    "        loss -= ax_i[y[i]]\n",
    "        loss += np.log(np.sum(np.exp(ax_i)))\n",
    "    loss = loss/N\n",
    "    \n",
    "    return loss\n",
    "    pass\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "\tSame as self.loss(X, y), except that it also returns the gradient.\n",
    "\n",
    "\tOutput: grad -- a matrix of the same dimensions as W containing \n",
    "\t\tthe gradient of the loss with respect to W.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    grad = np.zeros_like(self.W)\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and the gradient. Store the gradient\n",
    "    #   as the variable grad.\n",
    "    # ================================================================ #\n",
    "    N = X.shape[0]\n",
    "    C = self.W.shape[0]\n",
    "    loss = 0\n",
    "    grad = np.zeros_like(self.W)\n",
    "    \n",
    "    for i in range(N):\n",
    "        ax_i = self.W.dot(X[i])\n",
    "        loss -= ax_i[y[i]]\n",
    "        sum_i = np.sum(np.exp(ax_i))\n",
    "        loss += np.log(sum_i)\n",
    "        \n",
    "        for j in range(C):\n",
    "            p = np.exp(ax_i[j])/sum_i\n",
    "            grad[j, :] += p * X[i]\n",
    "            if j == y[i]:\n",
    "                grad[j, :] -= X[i]\n",
    "\n",
    "    loss = loss/N\n",
    "    grad = grad/N    \n",
    "       \n",
    "    pass\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def grad_check_sparse(self, X, y, your_grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in these dimensions.\n",
    "    \"\"\"\n",
    "  \n",
    "    for i in np.arange(num_checks):\n",
    "      ix = tuple([np.random.randint(m) for m in self.W.shape])\n",
    "  \n",
    "      oldval = self.W[ix]\n",
    "      self.W[ix] = oldval + h # increment by h\n",
    "      fxph = self.loss(X, y)\n",
    "      self.W[ix] = oldval - h # decrement by h\n",
    "      fxmh = self.loss(X,y) # evaluate f(x - h)\n",
    "      self.W[ix] = oldval # reset\n",
    "  \n",
    "      grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "      grad_analytic = your_grad[ix]\n",
    "      rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "      print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "\n",
    "  def fast_loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "    A vectorized implementation of loss_and_grad. It shares the same\n",
    "\tinputs and ouptuts as loss_and_grad.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(self.W.shape) # initialize the gradient as zero\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and gradient WITHOUT any for loops.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    A_x = np.dot(self.W, X.T)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    y_mat = np.zeros_like(A_x)\n",
    "    y_mat[y, range(N)] = 1   #the row that in y will be 1\n",
    "    \n",
    "    correct_wx = np.multiply(y_mat, A_x)\n",
    " \n",
    "    sums_wy = np.sum(correct_wx, axis=0) \n",
    "\n",
    "    exp_A_x = np.exp(A_x)\n",
    "    res = np.log(np.sum(exp_A_x, axis=0))\n",
    "    res -= sums_wy\n",
    "\n",
    "    loss = np.sum(res)\n",
    "\n",
    "    loss /= float(N)\n",
    "\n",
    "    ###grad \n",
    "\n",
    "    grad = exp_A_x / np.sum(exp_A_x, axis=0)\n",
    "    grad = np.dot(grad, X)\n",
    "    grad -= np.dot(y_mat, X)\n",
    "\n",
    "    grad /= float(N)  \n",
    "    \n",
    "    pass\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "\n",
    "    self.init_weights(dims=[np.max(y) + 1, X.shape[1]])\t# initializes the weights of self.W\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "\n",
    "    for it in np.arange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Sample batch_size elements from the training data for use in \n",
    "      #      gradient descent.  After sampling,\n",
    "      #     - X_batch should have shape: (dim, batch_size)\n",
    "      #     - y_batch should have shape: (batch_size,)\n",
    "      #   The indices should be randomly generated to reduce correlations\n",
    "      #   in the dataset.  Use np.random.choice.  It's okay to sample with\n",
    "      #   replacement.\n",
    "      # ================================================================ #\n",
    "\n",
    "      idx = np.random.choice(num_train, batch_size)\n",
    "      X_batch = X[idx]\n",
    "      y_batch = y[idx]\n",
    "\n",
    "      pass\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.fast_loss_and_grad(X_batch, y_batch)\n",
    "      loss_history.append(loss)\n",
    "      \n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Update the parameters, self.W, with a gradient step \n",
    "      # ================================================================ #\n",
    "      \n",
    "      self.W -= learning_rate * grad\n",
    "      pass\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Predict the labels given the training data.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    A_x = self.W.dot(X.T)\n",
    "    y_pred = np.argmax(A_x,axis = 0)\n",
    "    \n",
    "    pass\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
